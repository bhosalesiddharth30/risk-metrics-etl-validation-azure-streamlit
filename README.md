# Risk Metrics ETL & Validation Pipeline (Azure + Streamlit)

A fully functional data engineering pipeline for **risk metric ingestion, ETL processing, validation, and dashboard monitoring**, inspired by real workflows used in **energy trading**, **quant risk**, and **DevOps pipelines** at companies like **E.ON Energy Markets GmbH**.

This project simulates an **Azure Functionâ€“based ingestion system**, applies **ETL cleaning**, runs a **rule-based validation engine**, and displays results through an interactive **Streamlit dashboard**.

---

# ğŸ“Œ Project Overview

Energy trading and risk teams rely on daily metrics such as:

- **P50 / P95 forecasts**  
- **Spreads**  
- **Delta**  
- **Volatility clusters**

This system automates the workflow required to ingest, clean, validate, and monitor such metrics at scale.

### âœ” Highlights

- Simulated **Azure Function ingestion**
- ETL processing with **cleaning, normalization, date parsing**
- YAML-configured **risk validation rules**
- **Quality flags (GREEN/YELLOW/RED)** based on null ratio and metric rules
- Monitoring dashboard built with **Streamlit**
- Modular, production-inspired folder structure

---

# ğŸ— System Architecture (Mermaid Diagram)

```mermaid
flowchart TD

A[Sample_Data_Generator] --> B[Azure_Ingest_Function]
B --> C[ETL_Cleaner]
C --> D[Validation_Engine]
D --> E[Streamlit_Dashboard]
D --> F[Logger]
```
Below is the real output generated by this pipeline:


ğŸ“‚ Folder Structure
arduino
Copy code
azure-risk-metrics-etl-monitoring/
â”‚â”€â”€ dashboard_screenshot.png
â”‚â”€â”€ README.md
â”‚â”€â”€ requirements.txt
â”‚â”€â”€ generate_sample_data.py
â”‚â”€â”€ config/
â”‚   â””â”€â”€ validation_rules.yaml
â”‚â”€â”€ data/
â”‚   â”œâ”€â”€ sample_risk_data.csv
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â””â”€â”€ reports/
â”‚â”€â”€ dashboard/
â”‚   â””â”€â”€ streamlit_app.py
â”‚â”€â”€ etl/
â”‚   â””â”€â”€ risk_etl_cleaner.py
â”‚â”€â”€ functions/
â”‚   â””â”€â”€ azure_ingest_function.py
â”‚â”€â”€ utils/
â”‚   â””â”€â”€ logger.py
â”‚â”€â”€ validation/
â”‚   â””â”€â”€ risk_validation_engine.py
âš™ï¸ How It Works
1ï¸âƒ£ Sample Data Generator
Creates a 3-month dataset with:

3 portfolios (A/B/C)

Metrics: P50, P95, Spread, Delta, Vol

~1350 rows

Random missing values (simulating real risk data issues)

Run:

bash
Copy code
python generate_sample_data.py
2ï¸âƒ£ Simulated Azure Ingestion
Copies the daily snapshot into a raw/ folder, mimicking an Azure Function triggered by:

Blob upload

Timer events

ETRM pipelines

Run:

bash
Copy code
python -m functions.azure_ingest_function
3ï¸âƒ£ ETL Cleaner
Normalizes columns

Parses dates

Removes whitespace

Ensures consistent formatting

Run:

bash
Copy code
python -m etl.risk_etl_cleaner
4ï¸âƒ£ Validation Engine
Applies YAML rules:

Required columns

Min value thresholds

Null fraction limits per metric

Overall GREEN / YELLOW / RED flag

Run:

bash
Copy code
python -m validation.risk_validation_engine
Output written to:

data/processed/risk_snapshot_clean.csv

data/reports/validation_report.csv

5ï¸âƒ£ Streamlit Dashboard
Interactive monitoring UI:

Portfolio selector

Table & line chart visualizations

Validation summary

Overall quality status

Run:

bash
Copy code
streamlit run dashboard/streamlit_app.py
ğŸ¯ Why This Project Matters (E.ON Alignment)
This project directly matches responsibilities in the E.ON Energy Markets â€“ Working Student / Quantitative Risk / DevOps role:

âœ” Cloud workflows (Azure Function simulation)
âœ” Data pipelines (ETL â†’ processed datasets)
âœ” Risk metrics understanding (P50/P95/spread/delta/vol)
âœ” Data validation & governance
âœ” Dashboarding (Streamlit = similar to internal UIs)
âœ” YAML-based config (common in risk platforms)
âœ” Logging, structure, rule-based checks
This is exactly the type of real-world workload handled in energy trading risk & DevOps teams.

ğŸ“Œ Technologies Used
Python 3.10+

Pandas

NumPy

PyYAML

Streamlit

Loguru

Azure Function simulation

Rule-based validation engine

ğŸš€ Running Everything End-to-End
Step 1 â€” Install dependencies
bash
Copy code
pip install -r requirements.txt
Step 2 â€” Generate dataset
bash
Copy code
python generate_sample_data.py
Step 3 â€” Simulate ingestion
bash
Copy code
python -m functions.azure_ingest_function
Step 4 â€” ETL + validation
bash
Copy code
python -m validation.risk_validation_engine
Step 5 â€” Launch dashboard
bash
Copy code
streamlit run dashboard/streamlit_app.py
ğŸ“ License
MIT License.

ğŸ™Œ Author
Siddharth Sunil Bhosale
Master's in Artificial Intelligence Engineering (THI â€“ Germany)
Focus: Data Engineering, Quantitative Risk, ETL Pipelines, Energy Markets Analytics